gpu_id: [0]

exp_name: LaRa/release-test
n_views: 1
voxel_reso: 16

model:
    encoder_backbone: 'dinov2_vits14' # ['vit_small_patch16_224.dino','vit_base_patch16_224.dino', 'dinov2_vits14']

    n_groups: [4]  # n_groups for local attention
    # n_offset_groups: 32     # offset radius of 1/n_offset_groups of the scene size

    K: 4  # primitives per-voxel
    sh_degree: 1    # view dependent color

    # num_layers: 6 # 12
    num_heads: 16

    view_embed_dim: 32
    embedding_dim: 128

    label_in_channels: 128
    label_out_channels: 384

    # vol_feat_reso: 32
    vol_embedding_reso: ${voxel_reso}

    vol_embedding_out_dim: 80

    coarse_mlp_layers: 3

    img_feats_avg: True

    scene_size: 0.5
    opacity_shift: -2.1729
    scaling_shift: -5

    upsample_ratio: 2

    opacity_threshold: 0.02

    # ckpt_path: logs/LaRa/release-test/oclu-fine-16/vol_render_epoch=3999.ckpt # specify a ckpt path if you want to continue training  

triplane_e:
    cond_drop_prob: 0.1
    self_attn_heads: 16
    cross_attn_heads: 8
    init_scale: 0.25
    # n_ctx: 256
    embedding_channels: 128
    input_channels: 128
    output_channels: 128
    width: 256
    crossattndit_block_depth: 4

feature_vol_upsampler:
    in_channels: 128
    out_channels: 128
    kernel_size: 2
    stride: 2

train_dataset:
    dataset_name: objectron # shapenet_temp housecat6d
    data_root: /home/q672126/project/anything6d/ojectron_instances # train_on_one_category
    split: train
    img_size: [420, 420] # image resolution, [852,1096]
    n_group: ${n_views}
    voxel_reso: ${voxel_reso}
    n_scenes: 40
    load_normal: True
    positional_labelling: False
    clip_labelling: True
    labelling_dimension: 128

test_dataset:
    dataset_name: objectron # shapenet_temp housecat6d
    data_root: /home/q672126/project/anything6d/ojectron_instances # val_on_one_category
    split: test
    img_size: [420, 420]
    n_group: ${n_views}
    voxel_reso: ${voxel_reso}
    n_scenes: 40
    load_normal: True
    positional_labelling: False
    clip_labelling: True
    labelling_dimension: 128

train:
    batch_size: 2
    lr: 2e-4
    beta1: 0.9
    beta2: 0.95
    weight_decay: 0.05
    # betas: [0.9, 0.95]
    warmup_iters: 0
    n_epoch: 5000
    limit_train_batches: 1.0 # 0.2
    limit_val_batches: 1.0 # 0.02
    save_ckpt_every_n_epoch: 10
    check_val_every_n_epoch: 5000
    log_train_every_n_step: 10
    start_save: 2000
    start_fine: 2000
    lambda_normal: 0.2
    start_normal: 35000
test:
    batch_size: 2
    log_val_every_n_step: 10

logger: 
    name: wandb
    dir: logs/${exp_name}
